\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{von1791mechanismus}
\citation{fant1970acoustic}
\citation{klatt1980software}
\citation{tan2021survey}
\citation{zen2009statistical}
\citation{tan2021survey}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{4}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{ze2013statistical}
\citation{arik2017deep}
\citation{oord2016wavenet}
\citation{krizhevsky2017imagenet}
\citation{bojarski2016end}
\citation{zhou2018voxelnet}
\citation{oord2016wavenet}
\citation{kalchbrenner2018efficient}
\citation{paine2016fast}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces \def 1{1.25}\normalsize  \relax \def 1{1.25}\normalsize  Schematic overview of common text-to-speech components. The three blocks are typical intermediate steps, where an example of intermediate representations are shown above. The input text which speech is to be generated for is first analyzed to produce linguistic features. The example shows International Phonetic Alphabet (IPA) Symbols generated for the text \textit  {"In good printing"}. These are then fed into an acoustic model outputting acoustic features, in this case a mel spectrogram. Lastly the acoustic features are converted into audio using a vocoder model.}}{5}{figure.1.1}\protected@file@percent }
\newlabel{fig:TTS}{{1.1}{5}{\onehalfspacing Schematic overview of common text-to-speech components. The three blocks are typical intermediate steps, where an example of intermediate representations are shown above. The input text which speech is to be generated for is first analyzed to produce linguistic features. The example shows International Phonetic Alphabet (IPA) Symbols generated for the text \textit {"In good printing"}. These are then fed into an acoustic model outputting acoustic features, in this case a mel spectrogram. Lastly the acoustic features are converted into audio using a vocoder model}{figure.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Previous Work}{5}{section.1.1}\protected@file@percent }
\citation{wang2017tacotron}
\citation{shen2018natural}
\citation{rezende2015variational}
\citation{prenger2019waveglow}
\citation{kingma2018glow}
\citation{goodfellow2020generative}
\citation{donahue2018adversarial}
\citation{binkowski2019high}
\citation{kong2020hifi}
\citation{kong2020diffwave}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Problem statement}{7}{section.1.2}\protected@file@percent }
\newlabel{sec:statement}{{1.2}{7}{Problem statement}{section.1.2}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Theory}{8}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Sound \& Signals}{8}{section.2.1}\protected@file@percent }
\newlabel{sec:sounds}{{2.1}{8}{Sound \& Signals}{section.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}The Short-Time Fourier Transform}{9}{subsection.2.1.1}\protected@file@percent }
\newlabel{sec:stft}{{2.1.1}{9}{The Short-Time Fourier Transform}{subsection.2.1.1}{}}
\newlabel{eq:fouriert}{{2.1}{9}{The Short-Time Fourier Transform}{equation.2.1.1}{}}
\newlabel{eq:spec}{{2.4}{10}{The Short-Time Fourier Transform}{equation.2.1.4}{}}
\citation{stevens1937scale}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces \def 1{1.25}\normalsize  \relax \def 1{1.25}\normalsize  Waveform, Spectrogram, and Mel Spectrogram representation of the spoken sentence \textit  {"In good printing, the spaces between words should be as near as possible equal"}. The $x$-axis denotes the time in seconds, and the $y$-axis denotes the frequency- and mel-bins for the spectrogram and mel spectrogram respectively. Note that the mel spectrogram contains fewer bins than the spectrogram as a result of the choice of 80 mel filters, but that we observe a more detailed resolution for the speech components of the signal in the form of overtones. A brighter point represents a larger magnitude. Spectrograms were obtained identically as described in section \ref  {sec:method}.}}{11}{figure.2.1}\protected@file@percent }
\newlabel{fig:melspec}{{2.1}{11}{\onehalfspacing Waveform, Spectrogram, and Mel Spectrogram representation of the spoken sentence \textit {"In good printing, the spaces between words should be as near as possible equal"}. The $x$-axis denotes the time in seconds, and the $y$-axis denotes the frequency- and mel-bins for the spectrogram and mel spectrogram respectively. Note that the mel spectrogram contains fewer bins than the spectrogram as a result of the choice of 80 mel filters, but that we observe a more detailed resolution for the speech components of the signal in the form of overtones. A brighter point represents a larger magnitude. Spectrograms were obtained identically as described in section \ref {sec:method}}{figure.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}The Mel Spectrogram}{11}{subsection.2.1.2}\protected@file@percent }
\newlabel{sec:melspec}{{2.1.2}{11}{The Mel Spectrogram}{subsection.2.1.2}{}}
\citation{ma2013efficient}
\citation{dubnov2004generalization}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Spectral Flatness}{12}{subsection.2.1.3}\protected@file@percent }
\newlabel{sec:msf}{{2.1.3}{12}{Spectral Flatness}{subsection.2.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}The Phase Reconstruction Problem}{13}{subsection.2.1.4}\protected@file@percent }
\newlabel{sec:phaseprob}{{2.1.4}{13}{The Phase Reconstruction Problem}{subsection.2.1.4}{}}
\citation{kawahara1999restructuring}
\citation{agiomyrgiannakis2015vocaine}
\citation{hayes1980signal}
\citation{auger2012phase}
\citation{toda2007voice}
\citation{griffin1984signal}
\citation{wang2017tacotron}
\citation{prenger2019waveglow}
\citation{griffin1984signal}
\citation{albadawy2022vocbench}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces \def 1{1.25}\normalsize  \relax \def 1{1.25}\normalsize  The process of obtaining a mel spectrogram $\bm  {S}_{\text  {mel}}$ from an original signal $\bm  {x}$, and recreating it using a vocoder. The signal is first transformed using a Short-Time Fourier Transform, which is depicted using a bidirectional arrow to emphasize its invertibility. A spectrogram $\bm  {S}$ is then obtained though a magnitude transform, followed by a mel spectrogram $\bm  {S}_{\text  {mel}}$ via a transform $\mathcal  {M}$. These transforms are shown as unidirectional since they are non-invertible. Lastly the vocoder performs the inverse mapping to obtain $\tilde  {\bm  {x}}$. This procedure is followed during training of the vocoder model in order to generate training data, whereas during inference the mel spectrogram is generated by an acoustic model.}}{14}{figure.2.2}\protected@file@percent }
\newlabel{fig:phasere}{{2.2}{14}{\onehalfspacing The process of obtaining a mel spectrogram $\bm {S}_{\text {mel}}$ from an original signal $\bm {x}$, and recreating it using a vocoder. The signal is first transformed using a Short-Time Fourier Transform, which is depicted using a bidirectional arrow to emphasize its invertibility. A spectrogram $\bm {S}$ is then obtained though a magnitude transform, followed by a mel spectrogram $\bm {S}_{\text {mel}}$ via a transform $\mathcal {M}$. These transforms are shown as unidirectional since they are non-invertible. Lastly the vocoder performs the inverse mapping to obtain $\tilde {\bm {x}}$. This procedure is followed during training of the vocoder model in order to generate training data, whereas during inference the mel spectrogram is generated by an acoustic model}{figure.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}Griffin-Lim reconstruction}{14}{subsection.2.1.5}\protected@file@percent }
\citation{goodfellow2016deep}
\citation{foster2019generative}
\citation{baevski2020wav2vec}
\citation{devlin2018bert}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Learning Paradigms \& Generative Models}{15}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Neural Networks}{16}{subsection.2.2.1}\protected@file@percent }
\newlabel{eq:perc}{{2.8}{16}{Neural Networks}{equation.2.2.8}{}}
\citation{hornik1989multilayer}
\citation{lu2020universal}
\citation{lecun1989backpropagation}
\citation{goodfellow2016deep}
\citation{yu2015multi}
\citation{oord2016wavenet}
\citation{kong2020diffwave}
\citation{lee2021priorgrad}
\citation{oord2016wavenet}
\citation{tan2021survey}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces \def 1{1.25}\normalsize  \relax \def 1{1.25}\normalsize  A depiction of a simple neural network with two inputs, a single hidden layer with a width of three, and a single output. Each hidden value $h_k$ is calculated via equation (\ref  {eq:perc}). The right side of the figure shows the network written in vector form.}}{18}{figure.2.3}\protected@file@percent }
\newlabel{fig:NN}{{2.3}{18}{\onehalfspacing A depiction of a simple neural network with two inputs, a single hidden layer with a width of three, and a single output. Each hidden value $h_k$ is calculated via equation (\ref {eq:perc}). The right side of the figure shows the network written in vector form}{figure.2.3}{}}
\citation{goodfellow2016deep}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Learning Algorithm}{19}{subsection.2.2.2}\protected@file@percent }
\newlabel{sec:learn}{{2.2.2}{19}{Learning Algorithm}{subsection.2.2.2}{}}
\newlabel{eq:mle}{{2.11}{19}{Learning Algorithm}{equation.2.2.11}{}}
\newlabel{eq:datakl}{{2.13}{19}{Learning Algorithm}{equation.2.2.13}{}}
\newlabel{eq:nll}{{2.14}{19}{Learning Algorithm}{equation.2.2.14}{}}
\citation{kingma2014adam}
\citation{ruder2016overview}
\citation{paszke2019pytorch}
\newlabel{eq:loss}{{2.15}{20}{Learning Algorithm}{equation.2.2.15}{}}
\newlabel{eq:nabla}{{2.17}{20}{Learning Algorithm}{equation.2.2.17}{}}
\citation{Sohl-Dickstein_Weiss_Maheswaranathan_Ganguli_2015}
\citation{Sohl-Dickstein_Weiss_Maheswaranathan_Ganguli_2015}
\citation{neal2001annealed}
\citation{song2019generative}
\citation{kong2020diffwave}
\citation{kong2020diffwave}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Diffusion models}{21}{section.2.3}\protected@file@percent }
\newlabel{sec:diff}{{2.3}{21}{Diffusion models}{section.2.3}{}}
\newlabel{eq:kernel}{{2.21}{21}{Diffusion models}{equation.2.3.21}{}}
\citation{ho2020denoising}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces \def 1{1.25}\normalsize  \relax \def 1{1.25}\normalsize  The reverse and forward diffusion process for a given audio signal. Through $T$ diffusion steps a sample from a prior $q_{\text  {data}}$ is transformed into a sample from a distribution $p_{\text  {latent}}$. This process is to be reversed through a series of reverse transitional probabilities $p_{\theta }(\bm  {x}_{t-1} \mid \bm  {x}_{t})$. Image adopted from \cite  {kong2020diffwave}.}}{22}{figure.2.4}\protected@file@percent }
\newlabel{fig:diffwave1}{{2.4}{22}{\onehalfspacing The reverse and forward diffusion process for a given audio signal. Through $T$ diffusion steps a sample from a prior $q_{\text {data}}$ is transformed into a sample from a distribution $p_{\text {latent}}$. This process is to be reversed through a series of reverse transitional probabilities $p_{\theta }(\bm {x}_{t-1} \mid \bm {x}_{t})$. Image adopted from \cite {kong2020diffwave}}{figure.2.4}{}}
\newlabel{eq:abar}{{2.22}{22}{Diffusion models}{equation.2.3.22}{}}
\newlabel{eq:diffxt}{{2.24}{22}{Diffusion models}{equation.2.3.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces \def 1{1.25}\normalsize  \relax \def 1{1.25}\normalsize  An audio clip, corresponding mel spectrogram, and an image for different values of $\bar  {\alpha }_t$ (as defined in equation \ref  {eq:abar}) and schedules. The image was chosen arbitrarily to show the difference between the forward process for audio and images. The samples on the left side were generated using a scaled linear schedule with $\beta _T=0.02$, and the right side with an inverse quadratic schedule, as defined in equation \ref  {eq:sc-scaled} and \ref  {eq:sc-invquad}. The number on the leftmost side denotes the time step $t$. Ten linearly spaced samples are shown in each column, which were obtained using equation \ref  {eq:diffxt}.}}{23}{figure.2.5}\protected@file@percent }
\newlabel{fig:viper}{{2.5}{23}{\onehalfspacing An audio clip, corresponding mel spectrogram, and an image for different values of $\bar {\alpha }_t$ (as defined in equation \ref {eq:abar}) and schedules. The image was chosen arbitrarily to show the difference between the forward process for audio and images. The samples on the left side were generated using a scaled linear schedule with $\beta _T=0.02$, and the right side with an inverse quadratic schedule, as defined in equation \ref {eq:sc-scaled} and \ref {eq:sc-invquad}. The number on the leftmost side denotes the time step $t$. Ten linearly spaced samples are shown in each column, which were obtained using equation \ref {eq:diffxt}}{figure.2.5}{}}
\citation{ho2020denoising}
\citation{goodfellow2016deep}
\citation{ho2020denoising}
\citation{kong2020diffwave}
\newlabel{eq:ptheta}{{2.26}{24}{Diffusion models}{equation.2.3.26}{}}
\newlabel{eq:sigma}{{2.27}{24}{Diffusion models}{equation.2.3.27}{}}
\newlabel{vlb1}{{2.28}{24}{Diffusion models}{equation.2.3.28}{}}
\newlabel{eq:trainsimple}{{2.29}{24}{Diffusion models}{equation.2.3.29}{}}
\citation{kong2020diffwave}
\citation{chen2020wavegrad}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Training algorithm}}{25}{algorithm.1}\protected@file@percent }
\newlabel{alg:train}{{1}{25}{Diffusion models}{algorithm.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Sampling algorithm}}{25}{algorithm.2}\protected@file@percent }
\newlabel{alg:samp}{{2}{25}{Diffusion models}{algorithm.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Diffusion-based vocoders}{25}{subsection.2.3.1}\protected@file@percent }
\newlabel{sec:diffvoc}{{2.3.1}{25}{Diffusion-based vocoders}{subsection.2.3.1}{}}
\newlabel{eq:vocloss}{{2.32}{25}{Diffusion-based vocoders}{equation.2.3.32}{}}
\citation{rethage2018wavenet}
\citation{vaswani2017attention}
\citation{kong2020diffwave}
\citation{kong2020diffwave}
\citation{kong2020diffwave}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces \def 1{1.25}\normalsize  \relax \def 1{1.25}\normalsize  Neural network architecture of DiffWave which models $\bm  {\varepsilon }_{\theta }: \mathbb  {R}^L \times \mathbb  {N} \times \mathbb  {R}^{K_{\text  {mel}} \times M_{\text  {mel}}} \to \mathbb  {R}^L$. The conditioner used is in the form of a mel spectrogram $\bm  {S}_{\text  {mel}}$, the input is in the form of an audio signal $\bm  {x}_t$, and the Diffusion-step embedding input is in the form of a time step $t$. For a more detailed explanation of the architecture, please see \cite  {kong2020diffwave}.}}{26}{figure.2.6}\protected@file@percent }
\newlabel{fig:diffwavearch}{{2.6}{26}{\onehalfspacing Neural network architecture of DiffWave which models $\bm {\varepsilon }_{\theta }: \mathbb {R}^L \times \mathbb {N} \times \mathbb {R}^{K_{\text {mel}} \times M_{\text {mel}}} \to \mathbb {R}^L$. The conditioner used is in the form of a mel spectrogram $\bm {S}_{\text {mel}}$, the input is in the form of an audio signal $\bm {x}_t$, and the Diffusion-step embedding input is in the form of a time step $t$. For a more detailed explanation of the architecture, please see \cite {kong2020diffwave}}{figure.2.6}{}}
\citation{popov2021grad}
\citation{ren2020fastspeech}
\citation{luo2021lightspeech}
\citation{albadawy2022vocbench}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Limitations}{27}{subsection.2.3.2}\protected@file@percent }
\newlabel{sec:limitations}{{2.3.2}{27}{Limitations}{subsection.2.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Variance Schedules}{27}{section*.3}\protected@file@percent }
\citation{lam2022bddm}
\citation{chen2020wavegrad}
\citation{nichol2021improved}
\citation{ho2020denoising}
\newlabel{eq:sc-linear}{{2.33}{28}{Variance Schedules}{equation.2.3.33}{}}
\newlabel{eq:sc-scaled}{{2.34}{28}{Variance Schedules}{equation.2.3.34}{}}
\newlabel{eq:sc-cos}{{2.35}{28}{Variance Schedules}{equation.2.3.35}{}}
\newlabel{eq:sc-invquad}{{2.36}{28}{Variance Schedules}{equation.2.3.36}{}}
\citation{nichol2021improved}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces \def 1{1.25}\normalsize  \relax \def 1{1.25}\normalsize  Variances $\beta _t$ and noise scales $\bar  {\alpha }_t$ for different schedules. The noise scale determines the weighting of the noise and original signal through equation \ref  {eq:diffxt}. The normalized diffusion step on the $x$-axis represents how close the sample is to $\bm  {x}_t$ at $0$, and noise at $1$. All schedules are shown for $T=50$ steps.}}{29}{figure.2.7}\protected@file@percent }
\newlabel{fig:schedules}{{2.7}{29}{\onehalfspacing Variances $\beta _t$ and noise scales $\bar {\alpha }_t$ for different schedules. The noise scale determines the weighting of the noise and original signal through equation \ref {eq:diffxt}. The normalized diffusion step on the $x$-axis represents how close the sample is to $\bm {x}_t$ at $0$, and noise at $1$. All schedules are shown for $T=50$ steps}{figure.2.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces \def 1{1.25}\normalsize  \relax \def 1{1.25}\normalsize  Variances $\beta _t$ and noise scales $\bar  {\alpha }_t$ for Linear and Scaled Linear schedules for different number of diffusion steps, with $\beta _T=0.02$. The left column shows how $\bar  {\alpha }_t$ changes as the number of diffusion steps increase for the linear schedule, essentially converging quicker towards 0, i.e. noise. Note that the $\beta _t$ for the linear schedule has the same shape for all different $T$ as defined in (\ref  {eq:sc-linear}). The right column similarly shows how the noise scale changes for a scaled schedule, which better maintains the original shape of $\bar  {\alpha }_t$ for different $T$. The $x$-axis displays the normalized diffusion step $t/T$.}}{29}{figure.2.8}\protected@file@percent }
\newlabel{fig:scaled}{{2.8}{29}{\onehalfspacing Variances $\beta _t$ and noise scales $\bar {\alpha }_t$ for Linear and Scaled Linear schedules for different number of diffusion steps, with $\beta _T=0.02$. The left column shows how $\bar {\alpha }_t$ changes as the number of diffusion steps increase for the linear schedule, essentially converging quicker towards 0, i.e. noise. Note that the $\beta _t$ for the linear schedule has the same shape for all different $T$ as defined in (\ref {eq:sc-linear}). The right column similarly shows how the noise scale changes for a scaled schedule, which better maintains the original shape of $\bar {\alpha }_t$ for different $T$. The $x$-axis displays the normalized diffusion step $t/T$}{figure.2.8}{}}
\citation{nichol2021improved}
\@writefile{toc}{\contentsline {subsubsection}{The Inverse Quadratic Schedule}{30}{section*.4}\protected@file@percent }
\newlabel{sec:invquad}{{2.3.2}{30}{The Inverse Quadratic Schedule}{section*.4}{}}
\citation{watson2021learning}
\citation{lam2022bddm}
\citation{jolicoeur2021gotta}
\citation{chen2020wavegrad}
\citation{chen2022infergrad}
\citation{lee2021priorgrad}
\citation{lee2021priorgrad}
\citation{lee2021priorgrad}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces \def 1{1.25}\normalsize  \relax \def 1{1.25}\normalsize  Mean Spectral Flatness for different schedules and diffusion steps, which essentially shows how quickly samples from each schedule tends towards white noise. For each time step $t$, 100 samples $\bm  {x}_t$ were created from the training set for each schedule via equation \ref  {eq:diffxt}. The MSF was then calculated for all of these samples and averaged, where the shaded region indicates the standard deviation. A larger mean SF indicates that the audio sample is closer to white noise. Is is apparent that the inverse quadratic and linear schedules produce audio which goes towards white noise at a similar rate, while the others get there quicker.}}{31}{figure.2.9}\protected@file@percent }
\newlabel{fig:flatness}{{2.9}{31}{\onehalfspacing Mean Spectral Flatness for different schedules and diffusion steps, which essentially shows how quickly samples from each schedule tends towards white noise. For each time step $t$, 100 samples $\bm {x}_t$ were created from the training set for each schedule via equation \ref {eq:diffxt}. The MSF was then calculated for all of these samples and averaged, where the shaded region indicates the standard deviation. A larger mean SF indicates that the audio sample is closer to white noise. Is is apparent that the inverse quadratic and linear schedules produce audio which goes towards white noise at a similar rate, while the others get there quicker}{figure.2.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{Noise Prior}{31}{section*.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces \def 1{1.25}\normalsize  \relax \def 1{1.25}\normalsize  Intuitive depiction of starting the sampling or reverse diffusion process from a more informed prior $\mathcal  {N}(\bm  {\mu }, \bm  {\Sigma })$ compared to the originally proposed $\mathcal  {N}(\bm  {0}, \bm  {I})$ in order to obtain a sample which is closer to the real data distribution $p_{\text  {data}}$ faster. Image adopted from \cite  {lee2021priorgrad}.}}{32}{figure.2.10}\protected@file@percent }
\newlabel{fig:priorgrad1}{{2.10}{32}{\onehalfspacing Intuitive depiction of starting the sampling or reverse diffusion process from a more informed prior $\mathcal {N}(\bm {\mu }, \bm {\Sigma })$ compared to the originally proposed $\mathcal {N}(\bm {0}, \bm {I})$ in order to obtain a sample which is closer to the real data distribution $p_{\text {data}}$ faster. Image adopted from \cite {lee2021priorgrad}}{figure.2.10}{}}
\newlabel{eq:priorloss}{{2.38}{32}{Noise Prior}{equation.2.3.38}{}}
\newlabel{eq:frameenergy}{{2.39}{32}{Noise Prior}{equation.2.3.39}{}}
\citation{ho2020denoising}
\citation{kong2020diffwave}
\citation{nichol2021improved}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces \def 1{1.25}\normalsize  \relax \def 1{1.25}\normalsize  The starting prior $\bm  {x}_T$ sampled as white noise as well as scaled with the frame level energy of the mel spectrogram. The right plot shows the target audio (orange) along with the prior $\bm  {x}_T \sim \mathcal  {N}(\bm  {x}_T \, ; \, \bm  {0}, \bm  {I})$, where the noise in the right plot has been scaled using the variance defined in (\ref  {eq:frameenergy}). The red line shows the values of the frame level energy. Note how the prior noise on the right more closely follows the variance of the target audio (orange).}}{33}{figure.2.11}\protected@file@percent }
\newlabel{fig:priornoise}{{2.11}{33}{\onehalfspacing The starting prior $\bm {x}_T$ sampled as white noise as well as scaled with the frame level energy of the mel spectrogram. The right plot shows the target audio (orange) along with the prior $\bm {x}_T \sim \mathcal {N}(\bm {x}_T \, ; \, \bm {0}, \bm {I})$, where the noise in the right plot has been scaled using the variance defined in (\ref {eq:frameenergy}). The red line shows the values of the frame level energy. Note how the prior noise on the right more closely follows the variance of the target audio (orange)}{figure.2.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{Importance Sampling}{33}{section*.6}\protected@file@percent }
\newlabel{eq:weighted}{{2.40}{33}{Importance Sampling}{equation.2.3.40}{}}
\citation{chen2020wavegrad}
\citation{oord2016wavenet}
\citation{kong2020diffwave}
\citation{wang2017tacotron}
\citation{streijl2016mean}
\citation{rix2001perceptual}
\citation{avila2019non}
\citation{theis2015note}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Evaluation}{34}{section.2.4}\protected@file@percent }
\newlabel{sec:eval}{{2.4}{34}{Evaluation}{section.2.4}{}}
\citation{kong2020hifi}
\citation{lee2021priorgrad}
\citation{yamamoto2020parallel}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Log-mel Spectrogram Mean Absolute Error (LS-MAE)}{35}{subsection.2.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Peak Signal-to-Noise Ratio (PSNR)}{35}{subsection.2.4.2}\protected@file@percent }
\citation{cooper2022generalization}
\citation{ragano2022comparison}
\citation{cooper2022generalization}
\citation{baevski2020wav2vec}
\citation{panayotov2015librispeech}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Multi-resolution STFT Error (MRSE)}{36}{subsection.2.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}Approximate MOS (AMOS)}{36}{subsection.2.4.4}\protected@file@percent }
\newlabel{sec:amos}{{2.4.4}{36}{Approximate MOS (AMOS)}{subsection.2.4.4}{}}
\citation{ljspeech17}
\citation{tan2021survey}
\citation{kong2020diffwave}
\citation{lee2021priorgrad}
\citation{ljspeech17}
\citation{ljspeech17}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Data}{37}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}LJSpeech}{37}{section.3.1}\protected@file@percent }
\citation{panayotov2015librispeech}
\citation{zen2019libritts}
\citation{ardila2019common}
\citation{zen2019libritts}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces \def 1{1.25}\normalsize  \relax \def 1{1.25}\normalsize  Distribution of audio clip lengths in the LJSpeech dataset \cite  {ljspeech17}. The clips range from 0 to 10 seconds with a majority being over 5 seconds long.}}{38}{figure.3.1}\protected@file@percent }
\newlabel{fig:my_label}{{3.1}{38}{\onehalfspacing Distribution of audio clip lengths in the LJSpeech dataset \cite {ljspeech17}. The clips range from 0 to 10 seconds with a majority being over 5 seconds long}{figure.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}LibriTTS}{38}{section.3.2}\protected@file@percent }
\citation{lee2021priorgrad}
\citation{chen2020wavegrad}
\citation{wandb}
\citation{kong2020diffwave}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Method \& Results}{39}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:method}{{4}{39}{Method \& Results}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Experiment Settings}{39}{section.4.1}\protected@file@percent }
\newlabel{sec:settings}{{4.1}{39}{Experiment Settings}{section.4.1}{}}
\citation{kong2020hifi}
\citation{steinmetz2020auraloss}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Variance Schedules}{40}{section.4.2}\protected@file@percent }
\newlabel{sec:varscheds}{{4.2}{40}{Variance Schedules}{section.4.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces \def 1{1.25}\normalsize  \relax \def 1{1.25}\normalsize  Test set metrics for different variance schedules. For each experiment a DiffWave model was trained for 500k iterations followed by evaluation on each test set. For details on each schedule, see section \ref  {sec:limitations}. Results using the Griffin-Lim vocoder are also included for comparison.}}{41}{table.4.1}\protected@file@percent }
\newlabel{table:scheds}{{4.1}{41}{\onehalfspacing Test set metrics for different variance schedules. For each experiment a DiffWave model was trained for 500k iterations followed by evaluation on each test set. For details on each schedule, see section \ref {sec:limitations}. Results using the Griffin-Lim vocoder are also included for comparison}{table.4.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces \def 1{1.25}\normalsize  \relax \def 1{1.25}\normalsize  Approximate Mean Opinion Scores (AMOS) for DiffWave models trained with different schedules. Each model was trained for 500k iterations, and then evaluated on the respective test sets. Generated samples were evaluated by the \textsc  {w2v\_small}-model, described in section \ref  {sec:amos}, which produced approximate quality score on a scale from 1 to 5. Presented values are mean test set scores with standard deviations.}}{41}{table.4.2}\protected@file@percent }
\newlabel{table:sched-amos}{{4.2}{41}{\onehalfspacing Approximate Mean Opinion Scores (AMOS) for DiffWave models trained with different schedules. Each model was trained for 500k iterations, and then evaluated on the respective test sets. Generated samples were evaluated by the \textsc {w2v\_small}-model, described in section \ref {sec:amos}, which produced approximate quality score on a scale from 1 to 5. Presented values are mean test set scores with standard deviations}{table.4.2}{}}
\citation{nichol2021improved}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Importance Sampling}{42}{section.4.3}\protected@file@percent }
\newlabel{sec:imps}{{4.3}{42}{Importance Sampling}{section.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces \def 1{1.25}\normalsize  \relax \def 1{1.25}\normalsize  Loss $\mathcal  {L}_t$ ($L_1$ version) per noise step $t$ averaged for 10 000 training steps of DiffWave, including standard deviation. Note that the average loss is higher for earlier steps in the forward diffusion process, i.e. when the data is close to the real distribution, as opposed to when it is close to the prior noise.}}{42}{figure.4.1}\protected@file@percent }
\newlabel{fig:timestep_loss}{{4.1}{42}{\onehalfspacing Loss $\mathcal {L}_t$ ($L_1$ version) per noise step $t$ averaged for 10 000 training steps of DiffWave, including standard deviation. Note that the average loss is higher for earlier steps in the forward diffusion process, i.e. when the data is close to the real distribution, as opposed to when it is close to the prior noise}{figure.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces \def 1{1.25}\normalsize  \relax \def 1{1.25}\normalsize  Plots showing how the method of time step sampling affects the variance of the loss. The left plot shows the training losses for DiffWave trained using uniform and importance sampling between iteration 200 000 and 400 000, and the right plot shows histograms of both losses. Both models were trained using the original linear variance schedule with $\beta _T=0.05$. The loss obtained using importance sampling shows a lower variance compared to regular sampling. Note that only a subset of 200 loss values was used to create the histogram.}}{43}{figure.4.2}\protected@file@percent }
\newlabel{fig:loss_var}{{4.2}{43}{\onehalfspacing Plots showing how the method of time step sampling affects the variance of the loss. The left plot shows the training losses for DiffWave trained using uniform and importance sampling between iteration 200 000 and 400 000, and the right plot shows histograms of both losses. Both models were trained using the original linear variance schedule with $\beta _T=0.05$. The loss obtained using importance sampling shows a lower variance compared to regular sampling. Note that only a subset of 200 loss values was used to create the histogram}{figure.4.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces \def 1{1.25}\normalsize  \relax \def 1{1.25}\normalsize  Objective metrics for the DiffWave base model using a linear schedule with $\beta _T=0.05$ and uniform sampling, as well as an identical model using importance sampling, as defined in section \ref  {sec:limitations}. Both models were trained for 500k iterations using $T=50$ diffusion steps. Metrics for samples generated with both $T=50$ and $T=6$ (fast) steps are shown.}}{43}{table.4.3}\protected@file@percent }
\newlabel{table:weightstats}{{4.3}{43}{\onehalfspacing Objective metrics for the DiffWave base model using a linear schedule with $\beta _T=0.05$ and uniform sampling, as well as an identical model using importance sampling, as defined in section \ref {sec:limitations}. Both models were trained for 500k iterations using $T=50$ diffusion steps. Metrics for samples generated with both $T=50$ and $T=6$ (fast) steps are shown}{table.4.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces \def 1{1.25}\normalsize  \relax \def 1{1.25}\normalsize  Predicted AMOS for the DiffWave base model using a linear schedule with $\beta _T=0.05$ and uniform sampling compared to a model using importance sampling. Training was done for 500k iterations and $T=50$ diffusion steps. Metrics for samples generated with both $T=50$ and $T=6$ (fast) steps are shown. Not that no significant difference is observed in terms of AMOS between the two models.}}{44}{table.4.4}\protected@file@percent }
\newlabel{table:weightamos}{{4.4}{44}{\onehalfspacing Predicted AMOS for the DiffWave base model using a linear schedule with $\beta _T=0.05$ and uniform sampling compared to a model using importance sampling. Training was done for 500k iterations and $T=50$ diffusion steps. Metrics for samples generated with both $T=50$ and $T=6$ (fast) steps are shown. Not that no significant difference is observed in terms of AMOS between the two models}{table.4.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Noise Prior}{44}{section.4.4}\protected@file@percent }
\newlabel{sec:noiseprior}{{4.4}{44}{Noise Prior}{section.4.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces \def 1{1.25}\normalsize  \relax \def 1{1.25}\normalsize  Objective metrics for PriorGrad trained using 500k iterations and base settings. Samples were generated using both $T=50$ and $6$ diffusion steps (denoted by "fast"). PriorGrad shows better performance for both LS-MAE and PSNR, but worse MRSE.}}{45}{table.4.5}\protected@file@percent }
\newlabel{table:priorstats}{{4.5}{45}{\onehalfspacing Objective metrics for PriorGrad trained using 500k iterations and base settings. Samples were generated using both $T=50$ and $6$ diffusion steps (denoted by "fast"). PriorGrad shows better performance for both LS-MAE and PSNR, but worse MRSE}{table.4.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.6}{\ignorespaces \def 1{1.25}\normalsize  \relax \def 1{1.25}\normalsize  Subjective AMOS for PriorGrad trained using 500k iterations and base settings. Samples were generated using both $T=50$ and $6$ diffusion steps (denoted by "fast"). Overall PriorGrad achieves higher means in the slower base model case, and otherwise similar or slightly lower means when using fast sampling.}}{45}{table.4.6}\protected@file@percent }
\newlabel{table:prioramos}{{4.6}{45}{\onehalfspacing Subjective AMOS for PriorGrad trained using 500k iterations and base settings. Samples were generated using both $T=50$ and $6$ diffusion steps (denoted by "fast"). Overall PriorGrad achieves higher means in the slower base model case, and otherwise similar or slightly lower means when using fast sampling}{table.4.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Model Size}{45}{section.4.5}\protected@file@percent }
\newlabel{sec:modelsize}{{4.5}{45}{Model Size}{section.4.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.7}{\ignorespaces \def 1{1.25}\normalsize  \relax \def 1{1.25}\normalsize  Objective metrics for Base and Small models using an inverse quadratic schedule and importance sampling, as well as respective models for comparison. Results for the Base models of 2.6M parameters are presented in the top half, and for the Small models of 1.8M parameters in the bottom half. Each model was trained for 500k iterations and then evaluated on both the LJ-test and Libri-test sets. Inference was done using both $T=50$ and $6$ (denoted as "fast") diffusion steps. Models not using an inverse quadratic (IQ) schedule and importance sampling (IS) were trained using a linear $\beta _T=0.05$ schedule.}}{46}{table.4.7}\protected@file@percent }
\newlabel{table:smallstats}{{4.7}{46}{\onehalfspacing Objective metrics for Base and Small models using an inverse quadratic schedule and importance sampling, as well as respective models for comparison. Results for the Base models of 2.6M parameters are presented in the top half, and for the Small models of 1.8M parameters in the bottom half. Each model was trained for 500k iterations and then evaluated on both the LJ-test and Libri-test sets. Inference was done using both $T=50$ and $6$ (denoted as "fast") diffusion steps. Models not using an inverse quadratic (IQ) schedule and importance sampling (IS) were trained using a linear $\beta _T=0.05$ schedule}{table.4.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.8}{\ignorespaces \def 1{1.25}\normalsize  \relax \def 1{1.25}\normalsize  AMOS values for Base and Small models trained with an inverse quadratic schedule and importance sampling, along with linear $\beta _T=0.05$ baseline models. Each model was trained for 500k iterations and then evaluated on both the LJ-test and Libri-test sets. Inference was done using both $T=50$ and $T=6$ (denoted as "fast") diffusion steps. The Base model consisted of 2.6M parameters, and the Small model 1.8M parameters. In order to compare inference speeds a real-time factor (RTF) is also included, which denotes the time it takes to generate one second of audio (in seconds).}}{47}{table.4.8}\protected@file@percent }
\newlabel{table:smallamos}{{4.8}{47}{\onehalfspacing AMOS values for Base and Small models trained with an inverse quadratic schedule and importance sampling, along with linear $\beta _T=0.05$ baseline models. Each model was trained for 500k iterations and then evaluated on both the LJ-test and Libri-test sets. Inference was done using both $T=50$ and $T=6$ (denoted as "fast") diffusion steps. The Base model consisted of 2.6M parameters, and the Small model 1.8M parameters. In order to compare inference speeds a real-time factor (RTF) is also included, which denotes the time it takes to generate one second of audio (in seconds)}{table.4.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Inference Speed}{47}{section.4.6}\protected@file@percent }
\newlabel{sec:speed}{{4.6}{47}{Inference Speed}{section.4.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.9}{\ignorespaces \def 1{1.25}\normalsize  \relax \def 1{1.25}\normalsize  Real Time Factor (RTF) for each vocoder model. RTF is defined as the ratio between the time it takes for a model to generate an audio sample, and the audio sample's duration. This means that if the RTF is less than one, the model is faster than real-time. Each model performed inference conditioned on a mel spectrogram obtained from an audio sample. The presented RTF is the mean RTF for all such audio samples in the test set.}}{48}{table.4.9}\protected@file@percent }
\newlabel{table:rtf}{{4.9}{48}{\onehalfspacing Real Time Factor (RTF) for each vocoder model. RTF is defined as the ratio between the time it takes for a model to generate an audio sample, and the audio sample's duration. This means that if the RTF is less than one, the model is faster than real-time. Each model performed inference conditioned on a mel spectrogram obtained from an audio sample. The presented RTF is the mean RTF for all such audio samples in the test set}{table.4.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces \def 1{1.25}\normalsize  \relax \def 1{1.25}\normalsize  Real-time factors on CPU and GPU for respective models presented in table \ref  {table:rtf}. Blue bars and the left y-axis correspond to CPU values, and orange bars and the right y-axis GPU values.}}{48}{figure.4.3}\protected@file@percent }
\newlabel{fig:rtfs}{{4.3}{48}{\onehalfspacing Real-time factors on CPU and GPU for respective models presented in table \ref {table:rtf}. Blue bars and the left y-axis correspond to CPU values, and orange bars and the right y-axis GPU values}{figure.4.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Longer Training}{49}{section.4.7}\protected@file@percent }
\newlabel{sec:longer}{{4.7}{49}{Longer Training}{section.4.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.10}{\ignorespaces \def 1{1.25}\normalsize  \relax \def 1{1.25}\normalsize  Objective metrics for models trained for 2.5M iterations. Samples used for evaluation from HiFi-GAN were generated using a pre-trained checkpoint, and longer trained models were trained by resuming training from earlier checkpoints. HiFi-GAN generated audio with a single forward pass of the generator, where the diffusion models used $T=50$ steps, and $6$ for the fast schedule. Augmented models were extended with an inverse quadratic schedule (IQ) and importance sampling (IS).}}{49}{table.4.10}\protected@file@percent }
\newlabel{table:longstats}{{4.10}{49}{\onehalfspacing Objective metrics for models trained for 2.5M iterations. Samples used for evaluation from HiFi-GAN were generated using a pre-trained checkpoint, and longer trained models were trained by resuming training from earlier checkpoints. HiFi-GAN generated audio with a single forward pass of the generator, where the diffusion models used $T=50$ steps, and $6$ for the fast schedule. Augmented models were extended with an inverse quadratic schedule (IQ) and importance sampling (IS)}{table.4.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.11}{\ignorespaces \def 1{1.25}\normalsize  \relax \def 1{1.25}\normalsize  AMOS for models trained for 2.5M iterations, along with scores for the ground truth data, HiFi-GAN, and a shorter trained model. Scores were predicted from the audio generated by each model on each test set. The last two models were trained using the inverse quadratic (IQ) schedule, and importance sampling (IS).}}{50}{table.4.11}\protected@file@percent }
\newlabel{table:bigamos}{{4.11}{50}{\onehalfspacing AMOS for models trained for 2.5M iterations, along with scores for the ground truth data, HiFi-GAN, and a shorter trained model. Scores were predicted from the audio generated by each model on each test set. The last two models were trained using the inverse quadratic (IQ) schedule, and importance sampling (IS)}{table.4.11}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Discussion}{51}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Variance Schedules}{51}{section.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Importance Sampling}{52}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Noise Prior}{52}{section.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Size, Speed, \& Training Time}{53}{section.5.4}\protected@file@percent }
\citation{nichol2021improved}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Evaluation Methods}{54}{section.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.6}General Observations}{54}{section.5.6}\protected@file@percent }
\newlabel{sec:genobs}{{5.6}{54}{General Observations}{section.5.6}{}}
\citation{song2019generative}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion}{56}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Future work}{56}{section.6.1}\protected@file@percent }
\newlabel{sec:futurework}{{6.1}{56}{Future work}{section.6.1}{}}
\bibstyle{plain}
\bibdata{refs}
\bibcite{agiomyrgiannakis2015vocaine}{1}
\bibcite{albadawy2022vocbench}{2}
\bibcite{ardila2019common}{3}
\bibcite{arik2017deep}{4}
\bibcite{auger2012phase}{5}
\bibcite{avila2019non}{6}
\bibcite{baevski2020wav2vec}{7}
\bibcite{wandb}{8}
\bibcite{binkowski2019high}{9}
\bibcite{bojarski2016end}{10}
\bibcite{chen2020wavegrad}{11}
\bibcite{chen2022infergrad}{12}
\bibcite{cooper2022generalization}{13}
\bibcite{devlin2018bert}{14}
\bibcite{donahue2018adversarial}{15}
\bibcite{dubnov2004generalization}{16}
\bibcite{fant1970acoustic}{17}
\bibcite{foster2019generative}{18}
\bibcite{goodfellow2016deep}{19}
\bibcite{goodfellow2020generative}{20}
\bibcite{griffin1984signal}{21}
\bibcite{hayes1980signal}{22}
\bibcite{ho2020denoising}{23}
\bibcite{hornik1989multilayer}{24}
\bibcite{ljspeech17}{25}
\bibcite{jolicoeur2021gotta}{26}
\bibcite{kalchbrenner2018efficient}{27}
\bibcite{kawahara1999restructuring}{28}
\bibcite{kingma2014adam}{29}
\bibcite{kingma2018glow}{30}
\bibcite{klatt1980software}{31}
\bibcite{kong2020hifi}{32}
\bibcite{kong2020diffwave}{33}
\bibcite{krizhevsky2017imagenet}{34}
\bibcite{lam2022bddm}{35}
\bibcite{lecun1989backpropagation}{36}
\bibcite{lee2021priorgrad}{37}
\bibcite{lu2020universal}{38}
\bibcite{luo2021lightspeech}{39}
\bibcite{ma2013efficient}{40}
\bibcite{neal2001annealed}{41}
\bibcite{nichol2021improved}{42}
\bibcite{oord2016wavenet}{43}
\bibcite{paine2016fast}{44}
\bibcite{panayotov2015librispeech}{45}
\bibcite{paszke2019pytorch}{46}
\bibcite{popov2021grad}{47}
\bibcite{prenger2019waveglow}{48}
\bibcite{ragano2022comparison}{49}
\bibcite{ren2020fastspeech}{50}
\bibcite{rethage2018wavenet}{51}
\bibcite{rezende2015variational}{52}
\bibcite{rix2001perceptual}{53}
\bibcite{ruder2016overview}{54}
\bibcite{shen2018natural}{55}
\bibcite{Sohl-Dickstein_Weiss_Maheswaranathan_Ganguli_2015}{56}
\bibcite{song2019generative}{57}
\bibcite{steinmetz2020auraloss}{58}
\bibcite{stevens1937scale}{59}
\bibcite{streijl2016mean}{60}
\bibcite{tan2021survey}{61}
\bibcite{theis2015note}{62}
\bibcite{toda2007voice}{63}
\bibcite{vaswani2017attention}{64}
\bibcite{von1791mechanismus}{65}
\bibcite{wang2017tacotron}{66}
\bibcite{watson2021learning}{67}
\bibcite{yamamoto2020parallel}{68}
\bibcite{yu2015multi}{69}
\bibcite{ze2013statistical}{70}
\bibcite{zen2019libritts}{71}
\bibcite{zen2009statistical}{72}
\bibcite{zhou2018voxelnet}{73}
\gdef \@abspage@last{64}
